{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module Imports\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "import os\n",
    "import cv2 as cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates alias for these classes\n",
    "BaseOptions = mp.tasks.BaseOptions\n",
    "GestureRecognizer = mp.tasks.vision.GestureRecognizer\n",
    "GestureRecognizerOptions = mp.tasks.vision.GestureRecognizerOptions\n",
    "GestureRecognizerResult = mp.tasks.vision.GestureRecognizerResult\n",
    "VisionRunningMode = mp.tasks.vision.RunningMode\n",
    "ClassifierOptions = mp.tasks.components.processors.ClassifierOptions\n",
    "\n",
    "# absolute path to the gesture_recognizer TFlite file\n",
    "model_path = os.getcwd() + \"/exported_model/gesture_recognizer.task\"\n",
    "# base model recognizer options in common with other recognizers\n",
    "base_opt= BaseOptions(model_asset_path=model_path)\n",
    "\n",
    "# Use OpenCV’s VideoCapture to start capturing from the webcam.\n",
    "cap = cv.VideoCapture(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a gesture recognizer instance with the live stream mode:\n",
    "def print_result(result: GestureRecognizerResult, output_image: mp.Image, timestamp_ms: int):\n",
    "    print('gesture recognition result: {}'.format(result))\n",
    "\n",
    "# sets up classifier options (not implemented, just here for future use)\n",
    "custom_classifier_options = ClassifierOptions() # https://developers.google.com/mediapipe/api/solutions/python/mp/tasks/components/processors/ClassifierOptions\n",
    "\n",
    "# sets up gesture recognizer options\n",
    "options = GestureRecognizerOptions(\n",
    "    base_options=base_opt,\n",
    "    running_mode=VisionRunningMode.LIVE_STREAM,\n",
    "    result_callback=print_result,\n",
    "    min_hand_detection_confidence=0.5\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_timestamp_ms = 0\n",
    "with GestureRecognizer.create_from_options(options) as recognizer:\n",
    "    # Create a loop to read the latest frame from the camera using VideoCapture#read()\n",
    "    while cap.isOpened():\n",
    "        # reads capture data and returns the frame\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Empty Frame\")\n",
    "            break\n",
    "                \n",
    "        # checks if escape key (q) has been pressed every 5 ms\n",
    "        if cv.waitKey(5) & 0xff == ord('q'):\n",
    "            break\n",
    "        # Convert the frame received from OpenCV to a MediaPipe’s Image object.\n",
    "        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=frame)\n",
    "\n",
    "        frame_timestamp_ms += 1\n",
    "        # Send live image data to perform gesture recognition.\n",
    "        # The results are accessible via the `result_callback` provided in\n",
    "        # the `GestureRecognizerOptions` object.\n",
    "        # The gesture recognizer must be created with the live stream mode.\n",
    "        recognizer.recognize_async(mp_image, frame_timestamp_ms)\n",
    "\n",
    "# ends capture\n",
    "cap.release()\n",
    "cv.destroyAllWindows()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
